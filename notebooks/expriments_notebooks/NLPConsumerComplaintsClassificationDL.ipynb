{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_addons numpy pandas tensorflow sklearn nltk spacy textblob gensim scipy seaborn matplotlib minio mlflow wordcloud boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe87988-4794-48f0-8e0e-c336a23489c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "from io import StringIO\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import itertools\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "tokenizer = ToktokTokenizer()\n",
    "# stopword_list = nltk.corpus.stopwords.words('english')   \n",
    "# stopword_list = nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e5f8c-fafc-48df-861c-4cb9e4b818fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob nltk gensim wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e397a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c3f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ac49c-80bf-48ad-b041-a7eaf7dcc803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from minio import Minio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35839aee-02f3-41b0-9997-1af90b582a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOST = \"http://mlflow:5500\"\n",
    "\n",
    "PROJECT_NAME = \"NLPTextClassification\"\n",
    "EXPERIMENT_NAME = \"DeepLearning\"\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL']='http://minio-ml-workshop:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID']='minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='minio123'\n",
    "os.environ['AWS_REGION']='us-east-1'\n",
    "os.environ['AWS_BUCKET_NAME']='raw-data-saeed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2201c73-0605-41b1-8f4b-924aae88fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ae1c9-5dcb-4bca-bbe2-7d7e026b4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_s3_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb98f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9de56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"consumer_complaints.csv\")\n",
    "csv_file = client.get_object(\"raw-data-saeed\", \"consumer_complaints.csv\")\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# df = df[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb218b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481cd7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ed83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35adaf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44137f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c265831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()/df.shape[0]*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['complaint_id','date_received','product','issue','company','state','submitted_via','company_response_to_consumer','timely_response','consumer_disputed?','consumer_complaint_narrative']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[pd.notnull(df1['consumer_complaint_narrative'])]\n",
    "# df1 =  df1[:10000]\n",
    "df1 = df1.sample(n = 20000)\n",
    "df1.reset_index(drop=True,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977d79f-7b67-4c1c-afbe-28eedb3d46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b7027",
   "metadata": {},
   "source": [
    "## EDA\n",
    "We’ll check the disribution of complaints by product category to understand which product received maximum complaints and \n",
    "\n",
    "other products which rarely receive complaints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fcb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(18,6))\n",
    "sns.countplot(x='product',data=df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232c716",
   "metadata": {},
   "source": [
    "From this plot we can see Debt Collection and Mortgage received maximum number of complaints\n",
    "\n",
    "We’ll now analyze the contingency table in form of plot to understand which product has more customer disputes on their complaints after resolving the issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f96d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df1['product'],df1['consumer_disputed?']).plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b131590",
   "metadata": {},
   "source": [
    "Not much of difference in proportion of disputes raised by complaint for each product category.\n",
    "\n",
    "Checking various plots to identify patterns within data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b25bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['date_received'] = pd.to_datetime(df1['date_received'])\n",
    "df1.date_received.min(),df1.date_received.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8928d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['month'] = df1['date_received'].dt.month\n",
    "sns.countplot(x='month',data=df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba145e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='timely_response',data=df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d971c",
   "metadata": {},
   "source": [
    "# Text Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921d6b5",
   "metadata": {},
   "source": [
    "## Converting Text data to Lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45970b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['consumer_complaint_narrative'] =df1['consumer_complaint_narrative'].apply(lambda x: ' '.join([i.lower() for i in x.split()]))\n",
    "df1['consumer_complaint_narrative'].sample(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d287b",
   "metadata": {},
   "source": [
    "## Removing Punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a372420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['consumer_complaint_narrative'] =df1['consumer_complaint_narrative'].str.replace(r'[^\\w\\s]',\"\")\n",
    "df1['consumer_complaint_narrative'].sample(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25df29",
   "metadata": {},
   "source": [
    "## Text standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Below, we used three normalizazion dictionaries from these links :\n",
    "# #http://www.hlt.utdallas.edu/~yangl/data/Text_Norm_Data_Release_Fei_Liu/\n",
    "# #http://people.eng.unimelb.edu.au/tbaldwin/etc/emnlp2012-lexnorm.tgz\n",
    "# #http://luululu.com/tweet/typo-corpus-r1.txt\n",
    "dico = {}\n",
    "dico1 = open('doc1.txt', 'rb')\n",
    "for word in dico1:\n",
    "    word = word.decode('utf8')\n",
    "    word = word.split()\n",
    "    dico[word[1]] = word[3]\n",
    "dico1.close()\n",
    "dico2 = open('doc2.txt', 'rb')\n",
    "for word in dico2:\n",
    "    word = word.decode('utf8')\n",
    "    word = word.split()\n",
    "    dico[word[0]] = word[1]\n",
    "dico2.close()\n",
    "dico3 = open('doc3.txt', 'rb')\n",
    "for word in dico3:\n",
    "    word = word.decode('utf8')\n",
    "    word = word.split()\n",
    "    dico[word[0]] = word[1]\n",
    "dico3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_std(words):\n",
    "    list_words = words.split()\n",
    "    for i in range(len(list_words)):\n",
    "        if list_words[i] in dico.keys():\n",
    "            list_words[i] = dico[list_words[i]]\n",
    "    return ' '.join(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce3550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['consumer_complaint_narrative'] = df1['consumer_complaint_narrative'].apply(txt_std)\n",
    "df1.consumer_complaint_narrative.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acaef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1['consumer_complaint_narrative'] = df1['consumer_complaint_narrative'].str.replace(r\"xx+\\s\",\"\")\n",
    "df1['consumer_complaint_narrative'].head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e07ed",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df1['consumer_complaint_narrative'] =df1['consumer_complaint_narrative'].apply(lambda x: ' '.join([i for i in x.split() if i not in stop]))\n",
    "df1['consumer_complaint_narrative'].head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9e607",
   "metadata": {},
   "source": [
    "## Correcting Spelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b025d-0a9d-4286-9509-17085c888909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# import spark_util\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "# # Auxiliar functions\n",
    "# def equivalent_type(f):\n",
    "#     if f == 'datetime64[ns]': return TimestampType()\n",
    "#     elif f == 'int64': return LongType()\n",
    "#     elif f == 'int32': return IntegerType()\n",
    "#     elif f == 'float64': return FloatType()\n",
    "#     else: return StringType()\n",
    "\n",
    "# def define_structure(string, format_type):\n",
    "#     try: typo = equivalent_type(format_type)\n",
    "#     except: typo = StringType()\n",
    "#     return StructField(string, typo)\n",
    "\n",
    "# # Given pandas dataframe, it will return a spark's dataframe.\n",
    "# def pandas_to_spark(pandas_df):\n",
    "#     columns = list(pandas_df.columns)\n",
    "#     types = list(pandas_df.dtypes)\n",
    "#     struct_list = []\n",
    "#     for column, typo in zip(columns, types): \n",
    "#       struct_list.append(define_structure(column, typo))\n",
    "#     p_schema = StructType(struct_list)\n",
    "#     return spark.createDataFrame(pandas_df, p_schema)\n",
    "\n",
    "# spark = spark_util.getOrCreateSparkSession(\"Hello from Notebook\")\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "# # nums = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
    "\n",
    "# # nums = spark.createDataFrame(df1, df1.info)\n",
    "# nums = pandas_to_spark(df1)\n",
    "# nums.printSchema()\n",
    "\n",
    "# def texblobb(x):\n",
    "# #     import subprocess\n",
    "# #     import sys\n",
    "# #     subprocess.check_call([sys.executable, \"-m\", \"pip3\", \"install\", \"textblob\",\"--user\"])\n",
    "# #     from textblob import TextBlob\n",
    "    \n",
    "#     return str(TextBlob(x['consumer_complaint_narrative']).correct())\n",
    "\n",
    "# nums.rdd.map(texblobb ).collect() \n",
    "\n",
    "\n",
    "# # from pyspark.sql.functions import col\n",
    "# # col(\"consumer_complaint_narrative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cf432-289c-4826-95dd-9854a6b9cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b029ee2-be63-4fde-a64a-74fdea49f46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ensure text is standardized before applying this step\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "for index, row in df1.iterrows():\n",
    "#     row['consumer_complaint_narrative'] =TextBlob(row['consumer_complaint_narrative']).correct() \n",
    "#     for i, row in df.iterrows():\n",
    "#     if <something>:\n",
    "    df1.at[index, 'consumer_complaint_narrative'] = TextBlob(df1.at[index, 'consumer_complaint_narrative']).correct() \n",
    "    if index%5000 ==0:\n",
    "        print(index)\n",
    "    \n",
    "# df1['consumer_complaint_narrative'] =df1['consumer_complaint_narrative'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "# df1.consumer_complaint_narrative.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17932da3-4193-47f5-8f0d-b389d9987c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### df1.iloc[:10]['consumer_complaint_narrative'] =df1.iloc[:10]['consumer_complaint_narrative'].apply(lambda x: str(TextBlob(x).correct()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabb275",
   "metadata": {},
   "source": [
    "## Lemmatizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd3acc-5999-488a-990c-80b9bf1af529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import Word\n",
    "\n",
    "\n",
    "\n",
    "# for index, row in df1.iterrows():\n",
    "# #     row['consumer_complaint_narrative'] =TextBlob(row['consumer_complaint_narrative']).correct() \n",
    "# #     for i, row in df.iterrows():\n",
    "# #     if <something>:\n",
    "#     df1.at[index, 'consumer_complaint_narrative'] = ' '.join([Word(i).lemmatize() for i in df1.at[index, 'consumer_complaint_narrative'].split()])\n",
    "#     if index%1000 ==0:\n",
    "#         print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "\n",
    "\n",
    "\n",
    "df1['consumer_complaint_narrative'] =df1['consumer_complaint_narrative'].apply(lambda x:' '.join([Word(i).lemmatize() for i in x.split()]))\n",
    "df1.consumer_complaint_narrative.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180deb5b-75bf-4b76-a9bd-563f43569e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(r'large_cc_data.csv')\n",
    "# Upload data.\n",
    "result = client.fput_object(\n",
    "   \"raw-data-saeed\", \"large_cc_data.csv\", \"large_cc_data.csv\",content_type=\"application/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edbdd4-2dd1-41d8-9a98-21fdd0179517",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = client.get_object(\"raw-data-saeed\", \"data.csv\")\n",
    "df1 = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e937c",
   "metadata": {},
   "source": [
    "# Word Cloud for all Product categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c572f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for product_name in df1['product'].unique():\n",
    "    print(product_name)\n",
    "    all_words = ' '.join([text for text in df1.loc[df1['product'].str.contains(product_name),'consumer_complaint_narrative']])\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad62b0d",
   "metadata": {},
   "source": [
    "### Train/Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcce9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(df1['consumer_complaint_narrative'], df1['product'],stratify=df1['product'], \n",
    "                                                    test_size=0.30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2077aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feature engineering of consumer complaint with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dea78e0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "##label encoding target variable\n",
    "enc = preprocessing.LabelEncoder()\n",
    "train_y = enc.fit_transform(train_y)\n",
    "valid_y = enc.fit_transform(valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf72886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##tf-idf verctor representation\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df1['consumer_complaint_narrative'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577c518",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f05618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_complaints = np.append(train_x.values,valid_x.values)\n",
    "tokenizer = Tokenizer(num_words=25000)\n",
    "tokenizer.fit_on_texts(train_x.values)#total_complaints\n",
    "train_sequences = tokenizer.texts_to_sequences(train_x.values)\n",
    "test_sequences = tokenizer.texts_to_sequences(valid_x.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_index = tokenizer.word_index# dictionary containing words and their index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH = max([len(c.split()) for c in total_complaints])\n",
    "MAX_SEQUENCE_LENGTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d594e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc = preprocessing.LabelEncoder()\n",
    "train_labels = enc.fit_transform(train_y)\n",
    "test_labels = enc.fit_transform(valid_y)\n",
    "\n",
    "print(enc.classes_)\n",
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(test_labels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254cb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7644c-1949-408f-a75b-12e66316b1b1",
   "metadata": {},
   "source": [
    "\n",
    "## CNN w/ Pre-trained word embeddings(GloVe)\n",
    "We’ll use pre-trained embeddings such as Glove which provides word based vector representation trained on a large corpus.\n",
    "\n",
    "It is trained on a dataset of one billion tokens (words) with a vocabulary of 400 thousand words. The glove has embedding vector sizes, including 50, 100, 200 and 300 dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0113f9-7cf3-480e-be8e-5bc6f1a16230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "f = client.get_object(\"raw-data-saeed\", \"glove.6B.50d.txt\")\n",
    "embeddings_index = {}\n",
    "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "# f = open( 'glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    # print(line.decode(\"utf-8\") )\n",
    "    line = line.decode(\"utf-8\")\n",
    "    # break\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c16ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# # f = client.get_object(\"raw-data-saeed\", \"glove.6B.50d.txt\")\n",
    "# embeddings_index = {}\n",
    "# # f = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "# f = open( 'glove.6B.50d.txt')\n",
    "# for line in f:\n",
    "#     # print(line)\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86779d7e",
   "metadata": {},
   "source": [
    "Now lets create the embedding matrix using the word indexer created from tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45826314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_DIM = 50\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42aea74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lets check the word embedded vector representation for token ‘loan’ in our embedding matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "[(k,v) for k,v in word_index.items() if v==4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eea10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[4]  ## word embedded vector representation for token 'loan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f142d",
   "metadata": {},
   "source": [
    "Now we load this embedding matrix into an Embedding layer using Sequential API to form a Convolutional NeuralNet model.\n",
    "Dropout is applied between the hidden layers to factor regularization and prevent overfitting of neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c9c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(128, 3, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(512, 9, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(9))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(11, activation=\"softmax\"))\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "#  # optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "#               # optimizer='rmsprop',#rmsprop\n",
    "#  metrics=['acc'])\n",
    "\n",
    "model.compile(optimizer=tfa.optimizers.Yogi(0.001),\n",
    "              # optimizer=tfa.optimizers.RectifiedAdam(0.001),\n",
    "    # loss=tfa.losses.TripletSemiHardLoss(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc']\n",
    "    # metrics=[tfa.metrics.MultiLabelConfusionMatrix(num_classes=11)]\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a1450-da79-4f95-a084-1aa9f7efd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437f12a-59c8-4d52-a315-276384a32c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.trainable = True\n",
    "# for layer in model.layers[:-12]:\n",
    "#     layer.trainable =  False\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbeda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, labels_train,\n",
    " batch_size=128,\n",
    " epochs=80,\n",
    " validation_data=(test_data, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2700363",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :CNN',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b20a43",
   "metadata": {},
   "source": [
    "After 3 epochs the CNN tends to be overfitting the training data and therefore we need to implement early stopping to prevent such instances of overfitting and tune the number of epochs during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d59b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on test data\n",
    "predicted=model.predict(test_data)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee56637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "print('precision: \\n{}'.format(precision))\n",
    "print('recall: \\n{}'.format(recall))\n",
    "print('fscore: \\n{}'.format(fscore))\n",
    "print('support: \\n{}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987be029-8061-4094-8a85-32afec8b68bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51290b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test, predicted.round(),target_names=df1['product'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090a549",
   "metadata": {},
   "source": [
    "Now, we’ll initialize our Embedding layer from scratch and learning its weights during training instead of using a pre-trained word embeddings and build a small 1D convnet to solve our classification problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82107e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Embedding layer requires the specification of the vocabulary size (vocab_size), \n",
    "#the size of the real-valued vector space EMBEDDING_DIM = 100,\n",
    "#and the maximum length of input documents max_length .\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "EMBEDDING_DIM = 50\n",
    "max_length = 394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,\n",
    " EMBEDDING_DIM,\n",
    " input_length=MAX_SEQUENCE_LENGTH\n",
    " ))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dense(11, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    " # optimizer=\"rmsprop\",\n",
    "              optimizer=tfa.optimizers.RectifiedAdam(0.001),\n",
    " metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, labels_train,\n",
    " batch_size=256,\n",
    " epochs=80,\n",
    " validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :CNN',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c741366",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training acc', 'Validation acc'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves :CNN',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on test data\n",
    "predicted=model.predict(test_data)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362bd73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afcf31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bidirectional LSTM\n",
    "EMBEDDING_DIM = 50\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True))\n",
    "model.add(Bidirectional(LSTM(100, dropout = 0.3, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(256, dropout = 0.3)))\n",
    "model.add(Dense(11,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023ef19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ef56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, labels_train,\n",
    " batch_size=64,\n",
    " epochs=80,\n",
    " validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9428b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :RNN - LSTM',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d876903",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training acc', 'Validation acc'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves :RNN - LSTM',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c922df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61836da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on test data\n",
    "predicted=model.predict(test_data)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89568132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "print('precision: \\n{}'.format(precision))\n",
    "print('recall: \\n{}'.format(recall))\n",
    "print('fscore: \\n{}'.format(fscore))\n",
    "print('support: \\n{}'.format(support))\n",
    "print(\"############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c09ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0558f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test, predicted.round(),target_names=df1['product'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92b61f",
   "metadata": {},
   "source": [
    "After hours of training we get good results with LSTM(type of recurrent neural network) compared to CNN. From the learning curves it is clear the model needs to be tuned for overfitting by selecting hyperparameters such as no of epochs via early stopping and dropout for regularization.\n",
    "\n",
    "We could further improve our final result by ensembling our xgboost and Neural network models by using Logistic Regression as our base model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b785252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(r'light_cc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32467f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b817c74-937e-4c1c-9e06-adc099ad2f2d",
   "metadata": {},
   "source": [
    "## Step1: \n",
    "### Loading the Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d159338",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba30ca0-5b04-4db3-bb64-faeaf8e9ae31",
   "metadata": {},
   "source": [
    "Build a BERT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e8669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tokenization\n",
    "# module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "# bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "module_url = 'https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-768_A-12/1'\n",
    "\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True, signature='tokens',signature_outputs_as_dict = True)\n",
    "\n",
    "# bert_layer = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/distilbert_en_uncased_L-6_H-768_A-12/1\", trainable=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab50096a-a488-4813-85e4-4199afa173c3",
   "metadata": {},
   "source": [
    "## Step2: \n",
    "### Understand the Problem Statement and import the Datasets\n",
    "\n",
    "I am using Git hub bugs prediction dataset and it is available in MachineHack platform. Our aim is to predict the bugs,features and questions based on GitHub titles and the text body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007ab4e-135a-48fc-b42d-09c268350220",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(df1['consumer_complaint_narrative'], df1['product'],stratify=df1['product'], \n",
    "                                                    test_size=0.30)\n",
    "\n",
    "\n",
    "##label encoding target variable\n",
    "enc = preprocessing.LabelEncoder()\n",
    "train_labels = enc.fit_transform(train_y)\n",
    "test_labels = enc.fit_transform(test_y)\n",
    "\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', train_x.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08e3f4-b4b2-42be-98bd-7c2a02c8917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ##label encoding target variable\n",
    "# enc = preprocessing.LabelEncoder()\n",
    "# train_y = enc.fit_transform(train_y)\n",
    "# test_y = enc.fit_transform(test_y)\n",
    "# test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dea750-884a-4ceb-95e0-872a04b9e39a",
   "metadata": {},
   "source": [
    "bert_layer## Step3: \n",
    "### Encoding the raw text\n",
    "Tokenization is a process to take raw texts and split into tokens, which are numeric data to represent words.\n",
    "\n",
    "Official BERT language models are pre-trained with WordPiece vocabulary and use, not just token embeddings, but also segment embeddings distinguish between sequences, which are in pairs, e.g. question answering examples. Position embeddings are needed in order to inject positional awareness into the BERT model as the attention mechanism does not consider positions in context evaluation.\n",
    "\n",
    "The important limitation of BERT to be aware of is that the maximum length of the sequence for BERT is 512 tokens. For shorter sequence input than the maximum allowed input size, we would need to add pad tokens [PAD]. On the other hand, if the sequence is longer, we need to cut the sequence. This BERT limitation on the maximum length of the sequence is something that you need to be aware of for longer text segments.\n",
    "\n",
    "Very important are also the so-called special tokens, e.g. [CLS] token and [SEP] tokens. The [CLS] token will be inserted at the beginning of the sequence, the [SEP] token is at the end. If we deal with sequence pairs we will add additional [SEP] token at the end of the last.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0990d-3db8-48b1-8079-efd526274b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer.resolved_object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1795123-2ac9-4fbc-8088-1bb8d448faed",
   "metadata": {},
   "source": [
    "Step 4: Build the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf82415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    out = tf.keras.layers.Dense(11, activation='softmax')(net)\n",
    "    print (out.shape)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158a774-a1c8-468a-b2c4-6a1ea866d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = MAX_SEQUENCE_LENGTH\n",
    "train_input = bert_encode(train_x, tokenizer, max_len=max_len)\n",
    "test_input = bert_encode(test_x, tokenizer, max_len=max_len)\n",
    "# enc = preprocessing.LabelEncoder()\n",
    "# train_labels = enc.fit_transform(train_y)\n",
    "# test_labels = enc.fit_transform(test_y)\n",
    "# train_labels = to_categorical(np.asarray(train_labels))\n",
    "# test_labels = to_categorical(np.asarray(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665efd71-995a-41f0-ad17-0931ac22c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1['consumer_complaint_narrative'], df1['product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6e97e-ac9f-42fd-958a-d13e55e3e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10061a5-51ad-48f4-9909-9795ed6840f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = MAX_SEQUENCE_LENGTH\n",
    "# train_input = bert_encode(df1.consumer_complaint_narrative.values, tokenizer, max_len=max_len)\n",
    "# # test_input = bert_encode(test.consumer_complaint_narrative.values, tokenizer, max_len=max_len)\n",
    "# enc = preprocessing.LabelEncoder()\n",
    "# train_labels = enc.fit_transform(df1.product.values)\n",
    "# # train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe69cfe-b76c-46f2-856b-bbc80275397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=11)\n",
    "# test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16c3cb-e264-40a2-ae0f-b9f64152ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e5c09-cea6-43a4-99e2-8eeecf00594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(bert_layer, max_len=max_len)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd527cf-b74d-439b-8f29-bdcd8605acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = True\n",
    "for layer in model.layers[:-6]:\n",
    "    layer.trainable =  False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847c6cb-6b65-4071-9ab0-a76c16f0f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels, \n",
    "    validation_data=(test_input, test_labels),\n",
    "    # validation_split=0.2,\n",
    "    epochs=3,\n",
    "    callbacks=[checkpoint, earlystopping],\n",
    "    batch_size=1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a56551-9f1b-4a14-af70-61ea3e38552b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e355ce5-abc0-4eb8-bfc9-e82efa0cb30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')\n",
    "test_pred = model.predict(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d1d5f-a856-41de-9fd1-56adb608eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = hub.load(\"https://tfhub.dev/jeongukjae/distilbert_en_uncased_preprocess/1\")\n",
    "tokenize = hub.KerasLayer(preprocessor.tokenize)\n",
    "bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126049c-8f49-4fa6-91f4-36f5f296ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = [tokenize(segment) for segment in text_inputs]\n",
    "encoder_inputs = bert_pack_inputs(tokenized_inputs)\n",
    "encoder_outputs = encoder(encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee7389-fb05-493b-ab75-f1ac2042f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69fd50e-99bd-43e6-8988-0708aa9b9d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6a554-1ad2-4c11-bfbf-394f56542297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from minio import Minio\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import  preprocessing\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8c702-85b3-460c-b621-330c4989dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOST = \"http://mlflow:5500\"\n",
    "\n",
    "PROJECT_NAME = \"NLPTextClassification\"\n",
    "EXPERIMENT_NAME = \"DeepLearning\"\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL']='http://minio-ml-workshop:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID']='minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='minio123'\n",
    "os.environ['AWS_REGION']='us-east-1'\n",
    "os.environ['AWS_BUCKET_NAME']='raw-data-saeed'\n",
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient\n",
    "client = get_s3_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8ac48-2a14-4e26-817e-6fcba7631e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = client.get_object(\"raw-data-saeed\", \"light_cc_data.csv\")\n",
    "df1 = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9745cd-07fe-4fa6-87ec-048ff23f3487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a8cef-b289-4751-bbd8-11dfa88764e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(df1['consumer_complaint_narrative'], df1['product'],stratify=df1['product'], \n",
    "                                                    test_size=0.30)\n",
    "\n",
    "\n",
    "##label encoding target variable\n",
    "enc = preprocessing.LabelEncoder()\n",
    "train_labels = enc.fit_transform(train_y)\n",
    "test_labels = enc.fit_transform(test_y)\n",
    "\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', train_x.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f8818-c527-4cb7-ac49-53d37d173e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 348\n",
    "maxlen = 348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee2d58-94cd-4b9f-a542-f25ffa20e4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec1bff-98e1-4b47-b672-821528614e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16430c5a-fb95-48e8-8a57-f28e1749489f",
   "metadata": {},
   "source": [
    "## Implement a Transformer block as a layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00193e0a-7335-4556-8e66-3ed9d98a080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5086aa-b97d-4d1a-bace-5868c7f08d7a",
   "metadata": {},
   "source": [
    "## Implement embedding layer\n",
    "Two seperate embedding layers, one for tokens, one for token index (positions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c343b9-b8de-4c44-8b06-18eac10df60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb80d7c-781e-4ad5-a6ff-47a08d83305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0ed906-36de-4fdf-bd96-c3a8ce2f1b05",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470cd61-f229-42bc-b34c-83ae165bb9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_size = 25000  # Only consider the top 20k words\n",
    "maxlen = 348  # Only consider the first 200 words of each movie review\n",
    "# (x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "\n",
    "# train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "# test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9182e-0d8e-4fca-a489-b216614352f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "total_complaints = np.append(train_x.values,test_x.values)\n",
    "tokenizer = Tokenizer(num_words=25000)\n",
    "tokenizer.fit_on_texts(train_x.values)#total_complaints\n",
    "train_sequences = tokenizer.texts_to_sequences(train_x.values)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_x.values)\n",
    "\n",
    "\n",
    "print(len(train_x), \"Training sequences\")\n",
    "print(len(test_x), \"Validation sequences\")\n",
    "train_input = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=maxlen, padding='post')\n",
    "test_input = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba74936-556f-4a37-a51f-d257224e6cc4",
   "metadata": {},
   "source": [
    "## Create classifier model using transformer layer\n",
    "Transformer layer outputs one vector for each time step of our input sequence. Here, we take the mean across all time steps and use a feed forward network on top of it to classify text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c322f14-26e3-418a-bc56-ec6b3125ef21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521baa1-6c79-4208-983a-20140466f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 11  # Number of attention heads\n",
    "ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "# x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(11, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18303410-c854-488b-9736-44ccf3eeaf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d3d3c-f1d7-47ad-b54f-8e8f794d77dd",
   "metadata": {},
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7535b9c-97ae-4579-b100-51572384a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    train_input, labels_train, batch_size=32, epochs=10, validation_data=(test_input, labels_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe75e00-62b7-4fff-a5d1-823f5d2ce2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa4667-f184-45c9-b205-1bb3505326e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=14)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Loss',fontsize=12)\n",
    "plt.title('Loss Curves :Transformer',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c221c8e-e9d4-4733-88c1-d06bc9981c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5dbeaf-eff2-4817-8152-fc97553b26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training acc', 'Validation acc'],fontsize=14)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.title('Accuracy Curves :Transformer',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7294e94b-07d2-4436-8bc1-7ea5e0695e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e77abf-ddd1-4a6d-8727-d94c151a712d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abe8f03c-6d88-46f1-bcbe-5df34c8c8dec",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a35fdb6-cc8c-42ca-a15d-6815de5f7538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10d220c55dad87bc296f91a276cffc08c658df4f112171a2982baf6251a25e4e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
