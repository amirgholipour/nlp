{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4870212e-8584-458a-8b68-6b7567dffd9d",
   "metadata": {},
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b4a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.14.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 25.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/app-root/lib/python3.8/site-packages (1.19.5)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.8/site-packages (1.3.3)\n",
      "Requirement already satisfied: tensorflow in /opt/app-root/lib/python3.8/site-packages (2.6.0)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 117.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy\n",
      "  Downloading spacy-3.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 26.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting textblob\n",
      "  Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 112.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.1 MB 31.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.7.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.4 MB 85.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "\u001b[K     |████████████████████████████████| 292 kB 118.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.4.3-cp38-cp38-manylinux1_x86_64.whl (10.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.3 MB 80.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: minio in /opt/app-root/lib/python3.8/site-packages (6.0.2)\n",
      "Requirement already satisfied: mlflow in /opt/app-root/lib/python3.8/site-packages (1.20.2)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.1-cp38-cp38-manylinux1_x86_64.whl (371 kB)\n",
      "\u001b[K     |████████████████████████████████| 371 kB 113.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/app-root/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/app-root/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: clang~=5.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (1.41.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: keras~=2.6 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (3.15.8)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/app-root/lib/python3.8/site-packages (from tensorflow) (0.14.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.8 MB 102.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2021.10.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[K     |████████████████████████████████| 764 kB 97.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/app-root/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 114.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/app-root/lib/python3.8/site-packages (from nltk) (4.60.0)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp38-cp38-manylinux2014_x86_64.whl (458 kB)\n",
      "\u001b[K     |████████████████████████████████| 458 kB 109.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/app-root/lib/python3.8/site-packages (from spacy) (3.0.2)\n",
      "Collecting thinc<8.1.0,>=8.0.9\n",
      "  Downloading thinc-8.0.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (632 kB)\n",
      "\u001b[K     |████████████████████████████████| 632 kB 109.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp38-cp38-manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 61.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 103.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/app-root/lib/python3.8/site-packages (from spacy) (2.25.1)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp38-cp38-manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp38-cp38-manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 115.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.8/site-packages (from spacy) (57.4.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp38-cp38-manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8 MB 43.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 96.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 102.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /opt/app-root/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 109.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: urllib3 in /opt/app-root/lib/python3.8/site-packages (from minio) (1.26.4)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib/python3.8/site-packages (from minio) (2020.12.5)\n",
      "Requirement already satisfied: configparser in /opt/app-root/lib/python3.8/site-packages (from minio) (5.0.2)\n",
      "Requirement already satisfied: sqlalchemy in /opt/app-root/lib/python3.8/site-packages (from mlflow) (1.4.23)\n",
      "Requirement already satisfied: prometheus-flask-exporter in /opt/app-root/lib/python3.8/site-packages (from mlflow) (0.18.3)\n",
      "Requirement already satisfied: sqlparse>=0.3.1 in /opt/app-root/lib/python3.8/site-packages (from mlflow) (0.4.2)\n",
      "Requirement already satisfied: gunicorn in /opt/app-root/lib/python3.8/site-packages (from mlflow) (20.1.0)\n",
      "Requirement already satisfied: databricks-cli>=0.8.7 in /opt/app-root/lib/python3.8/site-packages (from mlflow) (0.15.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/app-root/lib/python3.8/site-packages (from mlflow) (1.6.0)\n",
      "Requirement already satisfied: entrypoints in /opt/app-root/lib/python3.8/site-packages (from mlflow) (0.3)\n",
      "Requirement already satisfied: querystring-parser in /opt/app-root/lib/python3.8/site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /opt/app-root/lib/python3.8/site-packages (from mlflow) (3.1.14)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /opt/app-root/lib/python3.8/site-packages (from mlflow) (4.0.1)\n",
      "Requirement already satisfied: alembic<=1.4.1 in /opt/app-root/lib/python3.8/site-packages (from mlflow) (1.4.1)\n",
      "Requirement already satisfied: Flask in /opt/app-root/lib/python3.8/site-packages (from mlflow) (2.0.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.8/site-packages (from mlflow) (5.4.1)\n",
      "Requirement already satisfied: docker>=4.0.0 in /opt/app-root/lib/python3.8/site-packages (from mlflow) (5.0.3)\n",
      "Requirement already satisfied: python-editor>=0.3 in /opt/app-root/lib/python3.8/site-packages (from alembic<=1.4.1->mlflow) (1.0.4)\n",
      "Requirement already satisfied: Mako in /opt/app-root/lib/python3.8/site-packages (from alembic<=1.4.1->mlflow) (1.1.5)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /opt/app-root/lib/python3.8/site-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/app-root/lib/python3.8/site-packages (from docker>=4.0.0->mlflow) (0.58.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/app-root/lib/python3.8/site-packages (from gitpython>=2.1.0->mlflow) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/app-root/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (4.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/app-root/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow) (3.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/app-root/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/app-root/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/app-root/lib/python3.8/site-packages (from sqlalchemy->mlflow) (1.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.30.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/app-root/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/app-root/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/app-root/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/app-root/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /opt/app-root/lib/python3.8/site-packages (from Flask->mlflow) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.8/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/app-root/lib/python3.8/site-packages (from prometheus-flask-exporter->mlflow) (0.10.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=ce0b7ed5a7a7e4c66a58e29fa855213007a3ba784bbb31206041bdad3944effb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-t09abqxs/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, threadpoolctl, srsly, smart-open, scipy, regex, pydantic, preshed, pillow, kiwisolver, joblib, cycler, blis, typeguard, thinc, spacy-legacy, scikit-learn, pathy, nltk, matplotlib, wordcloud, textblob, tensorflow-addons, spacy, sklearn, seaborn, gensim\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\n",
      "Successfully installed blis-0.7.4 catalogue-2.0.6 cycler-0.10.0 cymem-2.0.5 gensim-4.1.2 joblib-1.1.0 kiwisolver-1.3.2 matplotlib-3.4.3 murmurhash-1.0.5 nltk-3.6.5 pathy-0.6.0 pillow-8.3.2 preshed-3.0.5 pydantic-1.8.2 regex-2021.10.8 scikit-learn-1.0 scipy-1.7.1 seaborn-0.11.2 sklearn-0.0 smart-open-5.2.1 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.1 tensorflow-addons-0.14.0 textblob-0.15.3 thinc-8.0.10 threadpoolctl-3.0.0 typeguard-2.13.0 typer-0.4.0 wasabi-0.8.2 wordcloud-1.8.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons numpy pandas tensorflow sklearn nltk spacy textblob gensim scipy seaborn matplotlib minio mlflow wordcloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4e1d0-9de3-4fce-b43d-3a2463510d19",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9230902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from wordcloud import WordCloudfrom wordcloud import WordCloud\n",
    "from io import StringIO\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import itertools\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import joblib\n",
    "\n",
    "import mlflow\n",
    "import warnings\n",
    "\n",
    "from minio import Minio\n",
    "import subprocess\n",
    "import ipynbname\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "# stopword_list = nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e4ed1-1cad-4341-81d7-fa8780fc89b6",
   "metadata": {},
   "source": [
    "# Define the config file for mlflow and minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e397a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ac49c-80bf-48ad-b041-a7eaf7dcc803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35839aee-02f3-41b0-9997-1af90b582a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"http://mlflow:5500\"\n",
    "\n",
    "PROJECT_NAME = \"NlpTc\"\n",
    "EXPERIMENT_NAME = \"NlpLstm\"\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL']='http://minio-ml-workshop:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID']='minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='minio123'\n",
    "os.environ['AWS_REGION']='us-east-1'\n",
    "os.environ['AWS_BUCKET_NAME']='raw-data-saeed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2201c73-0605-41b1-8f4b-924aae88fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb3ae1c9-5dcb-4bca-bbe2-7d7e026b4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_s3_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22deba8c-4667-4958-b114-93d5c3094fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af49b7ed-6635-42d8-8981-05ec30a53d0a",
   "metadata": {},
   "source": [
    "# Load MLFlow to track the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d419a40c-2a99-4b35-aa6c-708f8414dad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'NlpLstm' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "# from verta.utils import ModelAPI\n",
    "\n",
    "# Connect to local MLflow tracking server\n",
    "mlflow.set_tracking_uri(HOST)\n",
    "\n",
    "# Set the experiment name...\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "mlflow.tensorflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c860bf3b-002c-4a00-b789-41028de2098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_git_revision_hash():\n",
    "    return subprocess.check_output(['git', 'rev-parse', 'HEAD'])\n",
    "\n",
    "def get_git_revision_short_hash():\n",
    "    return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'])\n",
    "\n",
    "def get_git_remote():\n",
    "    return subprocess.check_output(['git', 'config', '--get', 'remote.origin.url'])\n",
    "\n",
    "def get_git_user():\n",
    "    return subprocess.check_output(['git', 'config', 'user.name'])\n",
    "\n",
    "def get_git_branch():\n",
    "    return subprocess.check_output(['git', 'branch', '--show-current'])\n",
    "\n",
    "def get_pip_freeze():\n",
    "    return subprocess.check_output(['pip', 'freeze']).splitlines()\n",
    "\n",
    "\n",
    "def record_details(mlflow):\n",
    "    \"\"\"\n",
    "    This method is the anchor poijt and more activiteis will go in it\n",
    "    :param mlflow:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(\"pip_freeze.txt\", \"wb\") as file:\n",
    "        for line in get_pip_freeze():\n",
    "            file.write(line)\n",
    "            file.write(bytes(\"\\n\", \"UTF-8\"))\n",
    "    mlflow.log_artifact(\"pip_freeze.txt\")\n",
    "    file.close()\n",
    "    mlflow.log_artifact(\"model.h5\", artifact_path=\"model\")\n",
    "    mlflow.log_artifact(\"tokenizer.pkl\", artifact_path=\"model\")\n",
    "    mlflow.log_artifact(\"labelencoder.pkl\", artifact_path=\"model\")\n",
    "    \n",
    "    os.remove(\"pip_freeze.txt\")\n",
    "    os.remove(\"model.h5\")\n",
    "    os.remove(\"tokenizer.pkl\")\n",
    "    os.remove(\"labelencoder.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "def mlflow_grid_search(methodtoexecute, methodarguments):\n",
    "    with mlflow.start_run(tags= {\n",
    "        \"mlflow.source.git.commit\" : get_git_revision_hash() ,\n",
    "        \"mlflow.user\": get_git_user(),\n",
    "        \"mlflow.source.git.repoURL\": get_git_remote(),\n",
    "        \"git_remote\": get_git_remote(),\n",
    "        \"mlflow.source.git.branch\": get_git_branch(),\n",
    "        \"mlflow.docker.image.name\": os.getenv(\"JUPYTER_IMAGE\", \"LOCAL\"),\n",
    "        \"mlflow.source.type\": \"NOTEBOOK\",\n",
    "#         \"mlflow.source.name\": ipynbname.name()\n",
    "    }) as run:\n",
    "        methodtoexecute(**methodarguments)\n",
    "        record_details(mlflow)\n",
    "\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff849801-acfa-4877-b518-edbac3972d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_logged_data(run_id):\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    data = client.get_run(run_id).data\n",
    "    tags = {k: v for k, v in data.tags.items() if not k.startswith(\"mlflow.\")}\n",
    "    artifacts = [f.path for f in client.list_artifacts(run_id, \"model\")]\n",
    "    return data.params, data.metrics, tags, artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2523b21-5c01-448b-b06c-ea39181c408e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0fb98f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /opt/app-\n",
      "[nltk_data]     root/src/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0046817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /opt/app-\n",
      "[nltk_data]     root/src/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9de56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aabb275",
   "metadata": {},
   "source": [
    "# Readinng the data\n",
    "Reading the data from S3 bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f0edbdd4-2dd1-41d8-9a98-21fdd0179517",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = client.get_object(\"raw-data-saeed\", \"data.csv\")\n",
    "df1 = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e937c",
   "metadata": {},
   "source": [
    "# Word Cloud for all Product categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf6e8e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53c572f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for product_name in df1['product'].unique():\n",
    "#     print(product_name)\n",
    "#     all_words = ' '.join([text for text in df1.loc[df1['product'].str.contains(product_name),'consumer_complaint_narrative']])\n",
    "    \n",
    "#     wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "#     plt.figure(figsize=(10, 7))\n",
    "#     plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b390824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 13)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad62b0d",
   "metadata": {},
   "source": [
    "### Train/Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "adcce9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(df1['consumer_complaint_narrative'], df1['product'],stratify=df1['product'], \n",
    "                                                    test_size=0.30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f0e09e0-0f78-4287-a330-34167e1386a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Credit reporting'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x = train_x\n",
    "ind = 2\n",
    "train_x.iloc[ind]\n",
    "train_y.iloc[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2077aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feature engineering of consumer complaint with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8dea78e0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bank account or service' 'Consumer Loan' 'Credit card'\n",
      " 'Credit reporting' 'Debt collection' 'Money transfers' 'Mortgage'\n",
      " 'Other financial service' 'Payday loan' 'Prepaid card' 'Student loan']\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([104,  79, 161, 212, 382,  19, 350,   2,  23,   8,  60]))\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([ 45,  33,  69,  91, 163,   8, 150,   1,  10,   4,  26]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##label encoding target variable\n",
    "enc = preprocessing.LabelEncoder()\n",
    "train_labels = enc.fit_transform(train_y)\n",
    "test_labels = enc.fit_transform(valid_y)\n",
    "\n",
    "print(enc.classes_)\n",
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(test_labels, return_counts=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf72886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##tf-idf verctor representation\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df1['consumer_complaint_narrative'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577c518",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "39f05618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6cf7ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_complaints = np.append(train_x.values,valid_x.values)\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(train_x.values)#total_complaints\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_x.values)\n",
    "test_sequences = tokenizer.texts_to_sequences(valid_x.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1bf6d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7003 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_index = tokenizer.word_index# dictionary containing words and their index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d9e2e84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH = max([len(c.split()) for c in total_complaints])\n",
    "MAX_SEQUENCE_LENGTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9d594e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 348)\n",
      "(600, 348)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "346cd056-11fa-4922-a51d-c88c6778254f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bank account or service', 'Consumer Loan', 'Credit card',\n",
       "       'Credit reporting', 'Debt collection', 'Money transfers',\n",
       "       'Mortgage', 'Other financial service', 'Payday loan',\n",
       "       'Prepaid card', 'Student loan'], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "254cb16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1400, 348)\n",
      "Shape of label tensor: (1400, 11)\n",
      "Shape of label tensor: (600, 11)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7644c-1949-408f-a75b-12e66316b1b1",
   "metadata": {},
   "source": [
    "\n",
    "## CNN w/ Pre-trained word embeddings(GloVe)\n",
    "We’ll use pre-trained embeddings such as Glove which provides word based vector representation trained on a large corpus.\n",
    "\n",
    "It is trained on a dataset of one billion tokens (words) with a vocabulary of 400 thousand words. The glove has embedding vector sizes, including 50, 100, 200 and 300 dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ca0113f9-7cf3-480e-be8e-5bc6f1a16230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "f = client.get_object(\"raw-data-saeed\", \"glove.6B.50d.txt\")\n",
    "embeddings_index = {}\n",
    "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "# f = open( 'glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    # print(line.decode(\"utf-8\") )\n",
    "    line = line.decode(\"utf-8\")\n",
    "    # break\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86779d7e",
   "metadata": {},
   "source": [
    "Now lets create the embedding matrix using the word indexer created from tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "45826314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_DIM = 50\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42aea74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lets check the word embedded vector representation for token ‘loan’ in our embedding matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e34e9df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loan', 4)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "[(k,v) for k,v in word_index.items() if v==4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d1eea10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93484002,  0.40450999,  0.10856   , -0.61953998, -0.69220001,\n",
       "        0.32119   , -0.70885003,  0.071233  , -0.33484   ,  0.77158999,\n",
       "       -0.050077  ,  1.14460003,  0.01926   , -1.02590001,  0.85535002,\n",
       "       -0.081615  ,  0.19649   , -0.051262  ,  0.40103999,  0.87255001,\n",
       "        0.95371002, -0.87009001, -0.81568998, -0.24765   , -1.44400001,\n",
       "       -0.88612998,  1.51440001, -0.014284  , -0.48023999, -0.32289001,\n",
       "        3.00580001,  0.49408999,  0.72916001,  0.60891002,  0.59543997,\n",
       "        0.49731001, -0.0057787 , -0.21278   ,  0.94937998, -2.16849995,\n",
       "        0.12593   , -0.56818998,  0.50354999,  0.013716  , -1.01310003,\n",
       "       -0.46805999,  0.17305   ,  1.62039995,  0.60404998,  0.063104  ])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[4]  ## word embedded vector representation for token 'loan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a4e3e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3afcf31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f1ed59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bidirectional LSTM\n",
    "EMBEDDING_DIM = 50\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True))\n",
    "model.add(Bidirectional(LSTM(100, dropout = 0.3, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(256, dropout = 0.3)))\n",
    "model.add(Dense(11,activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e2f4365a-3396-4189-aa0e-8867d43f91cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 348, 50)           350200    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 348, 200)          120800    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 512)               935936    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                5643      \n",
      "=================================================================\n",
      "Total params: 1,412,579\n",
      "Trainable params: 1,412,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "842d1bd2-f58e-43f2-b5bf-457ab5409106",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'model.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3579c40c-8a9b-4c44-bb07-bb2d8c17ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "050013c0-566d-418b-8ddb-cf0b18557507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_data, labels_train,\n",
    "#                  batch_size=64,\n",
    "#                  epochs=1,\n",
    "#                  validation_data=(test_data, labels_test),callbacks=[model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c39ff120-3763-4841-99be-fa8509637a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = tf.keras.models.load_model('model.h5')\n",
    "# ind =1\n",
    "# x = [train_data[ind]]\n",
    "# x = tf.constant(x, dtype=tf.int32)\n",
    "# y=tf.math.argmax(labels_train[ind])\n",
    "# print(y)\n",
    "# pred = tf.math.argmax(tf.sigmoid(model(tf.constant(x))),axis=1)\n",
    "# print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5023ef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "22/22 [==============================] - 31s 1s/step - loss: 0.0437 - acc: 0.9886 - val_loss: 1.4145 - val_acc: 0.7567\n",
      "Epoch 2/2\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.0344 - acc: 0.9871 - val_loss: 1.3957 - val_acc: 0.7533\n",
      "INFO:tensorflow:Assets written to: /tmp/tmphyzqhdyo/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmphyzqhdyo/model/data/model/assets\n",
      "WARNING:urllib3.connectionpool:Failed to parse headers (url=http://minio-ml-workshop:9000/mlflow/6/dc2e8eb34daf49b4bdd1a942979e3886/artifacts/tensorboard_logs/validation/events.out.tfevents.1634252898.jupyterhub-nb-fmasood.326.38.v2): [MissingHeaderBodySeparatorDefect()], unparsed data: 'HTTP/1.1 200 OK\\r\\nAccept-Ranges: bytes\\r\\nContent-Length: 0\\r\\nContent-Security-Policy: block-all-mixed-content\\r\\nETag: \"559a5aafbb4f7d3f83bd40e362213f9b\"\\r\\nServer: MinIO\\r\\nStrict-Transport-Security: max-age=31536000; includeSubDomains\\r\\nVary: Origin\\r\\nVary: Accept-Encoding\\r\\nX-Amz-Request-Id: 16AE086943126AAC\\r\\nX-Content-Type-Options: nosniff\\r\\nX-Xss-Protection: 1; mode=block\\r\\nDate: Thu, 14 Oct 2021 23:09:22 GMT\\r\\n\\r\\n'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/app-root/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 465, in _make_request\n",
      "    assert_header_parsing(httplib_response.msg)\n",
      "  File \"/opt/app-root/lib/python3.8/site-packages/urllib3/util/response.py\", line 91, in assert_header_parsing\n",
      "    raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)\n",
      "urllib3.exceptions.HeaderParsingError: [MissingHeaderBodySeparatorDefect()], unparsed data: 'HTTP/1.1 200 OK\\r\\nAccept-Ranges: bytes\\r\\nContent-Length: 0\\r\\nContent-Security-Policy: block-all-mixed-content\\r\\nETag: \"559a5aafbb4f7d3f83bd40e362213f9b\"\\r\\nServer: MinIO\\r\\nStrict-Transport-Security: max-age=31536000; includeSubDomains\\r\\nVary: Origin\\r\\nVary: Accept-Encoding\\r\\nX-Amz-Request-Id: 16AE086943126AAC\\r\\nX-Content-Type-Options: nosniff\\r\\nX-Xss-Protection: 1; mode=block\\r\\nDate: Thu, 14 Oct 2021 23:09:22 GMT\\r\\n\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(enc, 'labelencoder.pkl')  \n",
    "joblib.dump(tokenizer, 'tokenizer.pkl')  \n",
    "\n",
    "with mlflow.start_run(tags= {\n",
    "        \"mlflow.source.git.commit\" : get_git_revision_hash() ,\n",
    "        \"mlflow.user\": get_git_user(),\n",
    "        \"mlflow.source.git.repoURL\": get_git_remote(),\n",
    "        \"git_remote\": get_git_remote(),\n",
    "        \"mlflow.source.git.branch\": get_git_branch(),\n",
    "        \"mlflow.docker.image.name\": os.getenv(\"JUPYTER_IMAGE\", \"LOCAL\"),\n",
    "        \"mlflow.source.type\": \"NOTEBOOK\",\n",
    "#         \"mlflow.source.name\": ipynbname.name()\n",
    "    }) as run:\n",
    "        model.fit(train_data, labels_train,\n",
    "                 batch_size=64,\n",
    "                 epochs=2,\n",
    "                 validation_data=(test_data, labels_test),callbacks=[model_checkpoint_callback])\n",
    "        record_details(mlflow)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0f2f5ce7-e74f-49b4-af05-cbb0b9e2ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildModel():\n",
    "    '''\n",
    "    Build Lstm model for tensorflow\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    self.final_set:\n",
    "        Features for modeling purpose\n",
    "    self.labels:\n",
    "        Output labels of the features\n",
    "    enc: \n",
    "        Ordinal Encoder definition file\n",
    "    ohe:\n",
    "        One hot  Encoder definition file\n",
    "    '''\n",
    "    def __init__(self, emweights = embedding_matrix, EMBEDDING_DIM= 50,MAX_SEQUENCE_LENGTH= 348, loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc']):\n",
    "        self.weights = [emweights]\n",
    "        self.input_length = MAX_SEQUENCE_LENGTH\n",
    "        self.embeding_dim = EMBEDDING_DIM\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "        self.model = []\n",
    "        \n",
    "    def DefineModel(self):\n",
    "        '''\n",
    "        Define the model\n",
    "        ----------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self.model\n",
    "        '''\n",
    "        #Bidirectional LSTM\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(len(word_index) + 1,\n",
    "                                    self.embeding_dim,\n",
    "                                    weights=self.weights,\n",
    "                                    input_length=self.input_length ,\n",
    "                                    trainable=True))\n",
    "        self.model.add(Bidirectional(LSTM(100, dropout = 0.4, return_sequences=True)))\n",
    "        self.model.add(Bidirectional(LSTM(256, dropout = 0.4)))\n",
    "        self.model.add(Dense(11,activation='sigmoid'))\n",
    "        # return self.final_set,self.labels, self.enc, self.ohe,self.encoding_flag\n",
    "    def CompileModel(self,loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc']):\n",
    "        self.model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics)\n",
    "#         return self.model\n",
    "    def BuildModel(self,loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc']):\n",
    "        self.DefineModel()\n",
    "        self.CompileModel()\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b740adae-5c4f-4d0c-90ef-b8cfb9da8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BuildModel(emweights = embedding_matrix).BuildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8d516f5b-c95f-46f5-93c4-21f7b01e2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'model.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "46397240-0169-4333-8897-49a04e3447bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "22/22 [==============================] - 31s 1s/step - loss: 0.1060 - acc: 0.9693 - val_loss: 1.2154 - val_acc: 0.7633\n",
      "Epoch 2/5\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1138 - acc: 0.9593 - val_loss: 1.1599 - val_acc: 0.7667\n",
      "Epoch 3/5\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1016 - acc: 0.9643 - val_loss: 1.2222 - val_acc: 0.7550\n",
      "Epoch 4/5\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1018 - acc: 0.9671 - val_loss: 1.2851 - val_acc: 0.7467\n",
      "Epoch 5/5\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.0896 - acc: 0.9707 - val_loss: 1.1775 - val_acc: 0.7683\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpfayeoh0w/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpfayeoh0w/model/data/model/assets\n",
      "WARNING:urllib3.connectionpool:Failed to parse headers (url=http://minio-ml-workshop:9000/mlflow/6/b0c28af534c44193b66fe85d38ec5b6a/artifacts/tensorboard_logs/validation/events.out.tfevents.1634259739.jupyterhub-nb-fmasood.326.48.v2): [MissingHeaderBodySeparatorDefect()], unparsed data: 'HTTP/1.1 200 OK\\r\\nAccept-Ranges: bytes\\r\\nContent-Length: 0\\r\\nContent-Security-Policy: block-all-mixed-content\\r\\nETag: \"d22c236a0d8124d838665bf11ae54f63\"\\r\\nServer: MinIO\\r\\nStrict-Transport-Security: max-age=31536000; includeSubDomains\\r\\nVary: Origin\\r\\nVary: Accept-Encoding\\r\\nX-Amz-Request-Id: 16AE0EB70069557D\\r\\nX-Content-Type-Options: nosniff\\r\\nX-Xss-Protection: 1; mode=block\\r\\nDate: Fri, 15 Oct 2021 01:04:53 GMT\\r\\n\\r\\n'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/app-root/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 465, in _make_request\n",
      "    assert_header_parsing(httplib_response.msg)\n",
      "  File \"/opt/app-root/lib/python3.8/site-packages/urllib3/util/response.py\", line 91, in assert_header_parsing\n",
      "    raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)\n",
      "urllib3.exceptions.HeaderParsingError: [MissingHeaderBodySeparatorDefect()], unparsed data: 'HTTP/1.1 200 OK\\r\\nAccept-Ranges: bytes\\r\\nContent-Length: 0\\r\\nContent-Security-Policy: block-all-mixed-content\\r\\nETag: \"d22c236a0d8124d838665bf11ae54f63\"\\r\\nServer: MinIO\\r\\nStrict-Transport-Security: max-age=31536000; includeSubDomains\\r\\nVary: Origin\\r\\nVary: Accept-Encoding\\r\\nX-Amz-Request-Id: 16AE0EB70069557D\\r\\nX-Content-Type-Options: nosniff\\r\\nX-Xss-Protection: 1; mode=block\\r\\nDate: Fri, 15 Oct 2021 01:04:53 GMT\\r\\n\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(enc, 'labelencoder.pkl')  \n",
    "joblib.dump(tokenizer, 'tokenizer.pkl')  \n",
    "\n",
    "with mlflow.start_run(tags= {\n",
    "        \"mlflow.source.git.commit\" : get_git_revision_hash() ,\n",
    "        \"mlflow.user\": get_git_user(),\n",
    "        \"mlflow.source.git.repoURL\": get_git_remote(),\n",
    "        \"git_remote\": get_git_remote(),\n",
    "        \"mlflow.source.git.branch\": get_git_branch(),\n",
    "        \"mlflow.docker.image.name\": os.getenv(\"JUPYTER_IMAGE\", \"LOCAL\"),\n",
    "        \"mlflow.source.type\": \"NOTEBOOK\",\n",
    "#         \"mlflow.source.name\": ipynbname.name()\n",
    "    }) as run:\n",
    "        model2.fit(train_data, labels_train,\n",
    "                 batch_size=64,\n",
    "                 epochs=5,\n",
    "                 validation_data=(test_data, labels_test),callbacks=[model_checkpoint_callback])\n",
    "        record_details(mlflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b9f917fc-c479-4c3e-917b-3832e112b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://mlflow:5500\n",
      "retrieving model metadata from mlflow...\n",
      "mlflow.pyfunc.loaded_model:\n",
      "  artifact_path: model\n",
      "  flavor: mlflow.keras\n",
      "  run_id: 29b0d88e4848478d940c38a2ec30f985\n",
      "\n",
      "initializing connection to s3 server...\n",
      "download successful\n"
     ]
    }
   ],
   "source": [
    "# !pip install mlflow\n",
    "# !pip install minio\n",
    "# !pip install boto3\n",
    "# !pip install scikit-learn==0.24.2\n",
    "# !pip install openshift-client==1.0.13\n",
    "# !pip show mlflow\n",
    "# !pip show minio\n",
    "# !pip show boto3\n",
    "# !pip show scikit-learn\n",
    "# !pip show openshift-client\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "from minio import Minio\n",
    "import openshift as oc\n",
    "from jinja2 import Template\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL']='http://minio-ml-workshop:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID']='minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='minio123'\n",
    "os.environ['AWS_REGION']='us-east-1'\n",
    "os.environ['AWS_BUCKET_NAME']='mlflow'\n",
    "# os.environ['MODEL_NAME'] = 'rossdemo'\n",
    "# os.environ['MODEL_VERSION'] = '1'\n",
    "# os.environ['OPENSHIFT_CLIENT_PYTHON_DEFAULT_OC_PATH'] = '/tmp/oc'\n",
    "\n",
    "HOST = \"http://mlflow:5500\"\n",
    "\n",
    "model_name = 'lstmv18'\n",
    "model_version = '1'\n",
    "build_name = f\"seldon-model-{model_name}-v{model_version}\"\n",
    "\n",
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient\n",
    "\n",
    "\n",
    "def init():\n",
    "    mlflow.set_tracking_uri(HOST)\n",
    "    print(HOST)\n",
    "    # Set the experiment name...\n",
    "    #mlflow_client = mlflow.tracking.MlflowClient(HOST)\n",
    "\n",
    "    \n",
    "def download_artifacts():\n",
    "    print(\"retrieving model metadata from mlflow...\")\n",
    "    model = mlflow.pyfunc.load_model(\n",
    "        model_uri=f\"models:/{model_name}/{model_version}\"\n",
    "    )\n",
    "    print(model)\n",
    "    \n",
    "    run_id = model.metadata.run_id\n",
    "    experiment_id = mlflow.get_run(run_id).info.experiment_id\n",
    "    \n",
    "    print(\"initializing connection to s3 server...\")\n",
    "    minioClient = get_s3_server()\n",
    "\n",
    "#     artifact_location = mlflow.get_experiment_by_name('rossdemo').artifact_location\n",
    "#     print(\"downloading artifacts from s3 bucket \" + artifact_location)\n",
    "\n",
    "    data_file_model = minioClient.fget_object(\"mlflow\", f\"/{experiment_id}/{run_id}/artifacts/model/model.h5\", \"model.h5\")\n",
    "    data_file_model2 = minioClient.fget_object(\"mlflow\", f\"/{experiment_id}/{run_id}/artifacts/model/tokenizer.pkl\", \"tokenizer.pkl\")\n",
    "    data_file_model3 = minioClient.fget_object(\"mlflow\", f\"/{experiment_id}/{run_id}/artifacts/model/labelencoder.pkl\", \"labelencoder.pkl\")\n",
    "\n",
    "\n",
    "    #Using boto3 Download the files from mlflow, the file path is in the model meta\n",
    "    #write the files to the file system\n",
    "    print(\"download successful\")\n",
    "    \n",
    "    return run_id\n",
    "    \n",
    "        \n",
    "init()\n",
    "run_id = download_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0528899f-653a-428b-804e-42d93bc9f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "class Predictor(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = tf.keras.models.load_model('model.h5', compile=False)\n",
    "        self.labelencoder = joblib.load('labelencoder.pkl')\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X,features_names):\n",
    "        # data = request.get(\"data\", {}).get(\"ndarray\")\n",
    "        # mult_types_array = np.array(data, dtype=object)\n",
    "        print ('step1......')\n",
    "        print(X)\n",
    "        X = tf.constant(X)\n",
    "        print ('step2......')\n",
    "        print(X)\n",
    "#         result = self.model.predict(X)\n",
    "        try:\n",
    "            result = self.model.predict(X)\n",
    "        except Exception as e:\n",
    "            print(traceback.format_exception(*sys.exc_info()))\n",
    "            raise # reraises the exception\n",
    "        \n",
    "\n",
    "                \n",
    "        print ('step3......')\n",
    "        result = tf.sigmoid(result)\n",
    "        print ('step4......')\n",
    "        print(result)\n",
    "        result = tf.math.argmax(result,axis=1)\n",
    "        print ('step5......')\n",
    "        print(result)\n",
    "        print(result.shape)\n",
    "        \n",
    "        print(self.labelencoder.inverse_transform(result))\n",
    "        print ('step6......')\n",
    "        return json.dumps(result, cls=JsonSerializer)\n",
    "\n",
    "class JsonSerializer(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (\n",
    "        np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9c432a41-0312-4d66-a211-790f376afa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import json\n",
    "\n",
    "class Transformer(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = joblib.load('tokenizer.pkl')\n",
    "        \n",
    "    def transform_input(self, X, feature_names, meta):\n",
    "        # print(request)\n",
    "        X = X.get(\"data\", {}).get(\"ndarray\")\n",
    "        print(X)\n",
    "        output = self.tokenizer.texts_to_sequences(X)\n",
    "        print(X)\n",
    "        \n",
    "        print(output)\n",
    "        output = pad_sequences(output, maxlen=348,padding='post')\n",
    "        print(output)\n",
    "#         output = tf.constant(output)\n",
    "#         print(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# class Transformer(object):\n",
    "#     def __init__(self):\n",
    "#         self.ordinalencoder = joblib.load('ordinalencoder.pkl')\n",
    "        \n",
    "#     def transform_input(self, request):\n",
    "#         X = request.get(\"data\", {}).get(\"ndarray\")\n",
    "#         feature_names = request.get(\"data\", {}).get(\"names\")\n",
    "        \n",
    "#         df = pd.DataFrame(X, columns=feature_names)\n",
    "#         df = ordinalencoder.transform(df)\n",
    "#         df = onehotencoder.transform(df)\n",
    "#         train_sequences = tokenizer.texts_to_sequences(train_x.values)\n",
    "\n",
    "\n",
    "#         #df = df.drop(['customerID'], axis=1)\n",
    "#         return df.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5217fb07-2c5c-49ca-8dcf-166fd0cff520",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2b4cf93e-2a00-4e05-a293-61be811251ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trying since 2015 get judgment report sued company court granted vacated judgment 2015 notified bureau time delete judgment within last week called talked transition told take long time verify information received letter yesterday told cleared 2015 bureau put judgment report 5 day filed transition telling take 90 day get trying qualify purchase home victim incompetence'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 2\n",
    "train_y.iloc[ind]\n",
    "train_x.iloc[ind]\n",
    "train_y.iloc[ind]\n",
    "train_x.iloc[ind]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5d99c611-218f-4c0f-b9d5-7ad65a4f9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = {\"data\":\n",
    "  {\n",
    "\n",
    "\n",
    "        \"names\":\n",
    "            [\n",
    "              \"Debt collection\"\n",
    "            ],\n",
    "      \"ndarray\": [\"could longer pay enormous charge hired company nl take either nothing pay day loan company accept term get several letter week threatened take civil action get check\"]\n",
    "\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9b8b28e8-ab1c-4720-b50e-6d43517b9a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['could longer pay enormous charge hired company nl take either nothing pay day loan company accept term get several letter week threatened take civil action get check']\n",
      "['could longer pay enormous charge hired company nl take either nothing pay day loan company accept term get several letter week threatened take civil action get check']\n",
      "[[35, 263, 21, 49, 787, 14, 1549, 88, 454, 188, 21, 22, 4, 14, 469, 321, 25, 94, 18, 121, 859, 88, 1371, 201, 25, 50]]\n",
      "[[  35  263   21   49  787   14 1549   88  454  188   21   22    4   14\n",
      "   469  321   25   94   18  121  859   88 1371  201   25   50    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "ready_data = Transformer().transform_input(sample_data,sample_data,sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb64f6-5db3-4a29-86e5-8dc0c5356522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = tf.keras.models.load_model('model.h5')\n",
    "# ind =1\n",
    "# x = [train_data[ind]]\n",
    "# x = tf.constant(x, dtype=tf.int32)\n",
    "# y=tf.math.argmax(labels_train[ind])\n",
    "# print(y)\n",
    "# pred = tf.math.argmax(tf.sigmoid(model(tf.constant(x))),axis=1)\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c426e-40b7-4b79-90f3-fa8fab06010d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "994de0ac-7ef5-40c6-96c0-d2711897767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1......\n",
      "[[  35  263   21   49  787   14 1549   88  454  188   21   22    4   14\n",
      "   469  321   25   94   18  121  859   88 1371  201   25   50    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "step2......\n",
      "tf.Tensor(\n",
      "[[  35  263   21   49  787   14 1549   88  454  188   21   22    4   14\n",
      "   469  321   25   94   18  121  859   88 1371  201   25   50    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(1, 348), dtype=int32)\n",
      "step3......\n",
      "step4......\n",
      "tf.Tensor(\n",
      "[[0.54987675 0.65573645 0.61018395 0.7247341  0.7023231  0.524241\n",
      "  0.5480431  0.5062037  0.56399965 0.5029819  0.5152898 ]], shape=(1, 11), dtype=float32)\n",
      "step5......\n",
      "tf.Tensor([3], shape=(1,), dtype=int64)\n",
      "(1,)\n",
      "['Credit reporting']\n",
      "step6......\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type EagerTensor is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-2b65f8d503c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mready_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mready_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-141-bec90e3fadbb>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, features_names)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabelencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'step6......'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJsonSerializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mJsonSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     return cls(\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mskipkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ascii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.8/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.8/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m<ipython-input-141-bec90e3fadbb>\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib64/python3.8/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type EagerTensor is not JSON serializable"
     ]
    }
   ],
   "source": [
    "output = Predictor().predict(ready_data,ready_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8dba8-b327-469a-a2b9-95b52e40dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(ready_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ef56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, labels_train,\n",
    " batch_size=64,\n",
    " epochs=80,\n",
    " validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9428b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :RNN - LSTM',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d876903",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training acc', 'Validation acc'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves :RNN - LSTM',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c922df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61836da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions on test data\n",
    "predicted=model.predict(test_data)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89568132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "print('precision: \\n{}'.format(precision))\n",
    "print('recall: \\n{}'.format(recall))\n",
    "print('fscore: \\n{}'.format(fscore))\n",
    "print('support: \\n{}'.format(support))\n",
    "print(\"############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c09ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0558f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test, predicted.round(),target_names=df1['product'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92b61f",
   "metadata": {},
   "source": [
    "After hours of training we get good results with LSTM(type of recurrent neural network) compared to CNN. From the learning curves it is clear the model needs to be tuned for overfitting by selecting hyperparameters such as no of epochs via early stopping and dropout for regularization.\n",
    "\n",
    "We could further improve our final result by ensembling our xgboost and Neural network models by using Logistic Regression as our base model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a35fdb6-cc8c-42ca-a15d-6815de5f7538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10d220c55dad87bc296f91a276cffc08c658df4f112171a2982baf6251a25e4e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
